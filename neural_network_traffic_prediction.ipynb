{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "280d59db-0499-4454-a149-01b7ec86968b",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "%pip install typing_extensions==3.7.4.3\n",
    "%pip install torch==2.0.0\n",
    "%pip install torch-geometric"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import glob\n",
    "\n",
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql.types import StructType, StructField, StringType, LongType, DoubleType, TimestampType\n",
    "from pyspark.sql.functions import col, to_timestamp, date_format, explode, sequence, expr, coalesce, when, dayofmonth, to_date, lit, substring, concat\n",
    "from pyspark.sql import functions as F\n",
    "from pyspark.ml.feature import StringIndexer\n",
    "from pyspark import StorageLevel\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import torch_geometric.nn as pyg_nn\n",
    "from torch_geometric.data import Data\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.metrics import confusion_matrix, accuracy_score, f1_score, precision_score, recall_score, classification_report\n",
    "from sklearn.model_selection import train_test_split\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from collections import Counter\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "bb0cd5cf-6f80-45bb-8297-30b2be9ad305",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Inicializo la sesión de Spark\n",
    "spark = SparkSession.builder.appName(\"IA_Neural_Network\").getOrCreate()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "004d9f78-64e0-4a4f-91f4-a6a7f5a4b2e8",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Defino las rutas de los archivos\n",
    "taxi_zones_path = \"/FileStore/tables/taxi_zones_with_coordinates.csv\"\n",
    "taxi_zones_lookup_path = \"/FileStore/tables/taxi_zone_lookup.csv\"\n",
    "weather_df_path = \"/FileStore/tables/weather_nyc_2024.csv\"\n",
    "events_data_path = \"/FileStore/tables/events.csv\"\n",
    "\n",
    "# Cargo los archivos CSV sin esquema explícito para que Spark infiera el esquema\n",
    "taxi_zones_df = spark.read.csv(taxi_zones_path, header=True, inferSchema=True)\n",
    "taxi_zones_lookup_df = spark.read.csv(taxi_zones_lookup_path, header=True, inferSchema=True)\n",
    "weather_df = spark.read.csv(weather_df_path, header=True, inferSchema=True)\n",
    "events_df = spark.read.option(\"sep\", \";\").csv(events_data_path, header=True, inferSchema=True)\n",
    "\n",
    "# Lista de archivos a cargar\n",
    "file_paths = [\n",
    "    \"dbfs:/FileStore/tables/fhvhv_tripdata_2024_01.parquet\",\n",
    "    \"dbfs:/FileStore/tables/fhvhv_tripdata_2024_02.parquet\",\n",
    "    \"dbfs:/FileStore/tables/fhvhv_tripdata_2024_03.parquet\",\n",
    "    \"dbfs:/FileStore/tables/fhvhv_tripdata_2024_04.parquet\",\n",
    "    \"dbfs:/FileStore/tables/fhvhv_tripdata_2024_05.parquet\",\n",
    "    \"dbfs:/FileStore/tables/fhvhv_tripdata_2024_06.parquet\",\n",
    "]\n",
    "\n",
    "df_list = []\n",
    "for file_path in file_paths:\n",
    "    temp_df = spark.read.parquet(file_path)\n",
    "    df_list.append(temp_df)\n",
    "\n",
    "# Concateno todos los DataFrames de la lista\n",
    "uber_df = df_list[0]\n",
    "for df in df_list[1:]:\n",
    "    uber_df = uber_df.union(df)\n",
    "\n",
    "uber_df = uber_df.filter((dayofmonth(col(\"request_datetime\")) >= 1) & (dayofmonth(col(\"request_datetime\")) <= 31))\n",
    "\n",
    "# Imprimo en pantalla los esquemas inferidos\n",
    "taxi_zones_df.printSchema()\n",
    "taxi_zones_lookup_df.printSchema()\n",
    "uber_df.printSchema()\n",
    "\n",
    "# Cuento la cantidad de filas en el DataFrame concatenado\n",
    "print(f\"Cantidad de filas en el DataFrame concatenado: {uber_df.count()}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "48babffc-9157-401b-b356-3fe3e558ea24",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "uber_df = uber_df.sample(fraction=0.02, seed=42) # Sample 1% of the data\n",
    "# Verifico la cantidad de filas en el DataFrame concatenado\n",
    "print(f\"Cantidad de filas en el DataFrame concatenado: {uber_df.count()}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "c937333f-1149-48ba-8c03-a4a00b7ffa90",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "# Creo el filtro para las zonas de Manhattan\n",
    "manhattan_location_ids = taxi_zones_df.filter(col(\"borough\") == \"Manhattan\") \\\n",
    "                                       .select(\"LocationID\") \\\n",
    "                                       .rdd.flatMap(lambda x: x).collect()\n",
    "\n",
    "# Aplico el filtro en los datos de hide railing para quedarme solo las rutas que son completamente dentro de Manhattan\n",
    "uber_df = uber_df.filter(\n",
    "    (col(\"PULocationID\").isin(manhattan_location_ids) & col(\"DOLocationID\").isin(manhattan_location_ids))\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "d5ca24a5-3269-4c1c-bce4-61f7ef9392cf",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "print(f\"Cantidad de filas en el DataFrame concatenado: {uber_df.count()}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "1e0d67ac-dbdd-4560-a110-d76e08c6149d",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "\n",
    "# Configuro la política de parser de fechas\n",
    "spark.conf.set(\"spark.sql.legacy.timeParserPolicy\", \"LEGACY\")\n",
    "\n",
    "# Elimino duplicados\n",
    "events_df = events_df.dropDuplicates()\n",
    "\n",
    "# Uniformizo las columnas de fecha a tipo Timestamp, manejando ambos formatos de fecha\n",
    "events_df = events_df.withColumn(\n",
    "    \"Start Date/Time\",\n",
    "    coalesce(\n",
    "        to_timestamp(col(\"Start Date/Time\"), \"dd/MM/yyyy HH:mm\"),\n",
    "        to_timestamp(col(\"Start Date/Time\"), \"MM/dd/yyyy hh:mm:ss a\")\n",
    "    )\n",
    ").withColumn(\n",
    "    \"End Date/Time\",\n",
    "    coalesce(\n",
    "        to_timestamp(col(\"End Date/Time\"), \"dd/MM/yyyy HH:mm\"),\n",
    "        to_timestamp(col(\"End Date/Time\"), \"MM/dd/yyyy hh:mm:ss a\")\n",
    "    )\n",
    ")\n",
    "\n",
    "# Filtro filas donde End Date/Time es anterior a Start Date/Time (se entienden como valores atípicos o errores)\n",
    "events_df = events_df.filter(col(\"End Date/Time\") >= col(\"Start Date/Time\"))\n",
    "\n",
    "# Creo la secuencia de horas entre Start Date/Time y End Date/Time usando INTERVAL 1 HOUR\n",
    "events_with_hours_df = events_df.withColumn(\n",
    "    \"hour_sequence\",\n",
    "    explode(sequence(\n",
    "        col(\"Start Date/Time\"),\n",
    "        col(\"End Date/Time\"),\n",
    "        expr(\"INTERVAL 1 HOUR\")\n",
    "    ))\n",
    ")\n",
    "\n",
    "# Extraigo la fecha y la hora de la secuencia generada\n",
    "events_with_hours_df = events_with_hours_df.withColumn(\"Date\", date_format(col(\"hour_sequence\"), \"yyyy-MM-dd\")) \\\n",
    "                                           .withColumn(\"Hour\", date_format(col(\"hour_sequence\"), \"HH:00\"))\n",
    "\n",
    "# Creo una columna que marque si un evento ocurre en esa hora\n",
    "events_with_hours_df = events_with_hours_df.withColumn(\n",
    "    \"Event Count\", \n",
    "    when(col(\"Event Type\").isNotNull(), 1).otherwise(0)\n",
    ")\n",
    "\n",
    "# Agrupo por fecha, hora y tipo de evento, luego realizo un pivote para contar los eventos por tipo de evento\n",
    "aggregated_events_df = events_with_hours_df.groupBy(\"Date\", \"Hour\").pivot(\"Event Type\").agg({\"Event Count\": \"sum\"})\n",
    "\n",
    "# Relleno valores nulos con 0\n",
    "aggregated_events_df = aggregated_events_df.fillna(0)\n",
    "# Sumo todas las columnas que no son ni 'Date' ni 'Hour' para obtener la columna 'nº events'\n",
    "aggregated_events_df = aggregated_events_df.withColumn(\n",
    "    \"nº events\",\n",
    "    sum([F.col(col).cast(\"int\") for col in aggregated_events_df.columns if col not in [\"Date\", \"Hour\"]])\n",
    ")\n",
    "# imprimo en pantalla el resultado\n",
    "aggregated_events_df.show(10, truncate=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "94c7aaa9-e41a-47a2-ad7b-921c284eaac8",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Selecciono las columnas requeridas de taxi_zones_df\n",
    "taxi_zones_df_filtered = taxi_zones_df.select(\"LocationID\", \"borough\", \"lat\", \"lon\")\n",
    "\n",
    "# Reliazo un join left de uber_df con taxi_zones_df_filtered en la columna \"PULocationID\" (id de recogida)\n",
    "uber_df = uber_df.join(\n",
    "    taxi_zones_df_filtered,\n",
    "    uber_df[\"PULocationID\"] == taxi_zones_df_filtered[\"LocationID\"],\n",
    "    how=\"left\"\n",
    ").drop(\"LocationID\")  # Elimino el LocationID duplicado tras la unión\n",
    "\n",
    "# Renombro las columnas lat y lon como \"pickup_lat\" y \"pickup_lon\"\n",
    "uber_df = uber_df.withColumnRenamed(\"lat\", \"pickup_lat\").withColumnRenamed(\"lon\", \"pickup_lon\")\n",
    "\n",
    "# Imprimo en pantalla el resultado final\n",
    "uber_df.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "56eb367b-7593-462f-96b9-9febabe6763f",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Renombro las columnas en taxi_zones_df_filtered antes de la unión\n",
    "taxi_zones_df_filtered = taxi_zones_df.select(\"LocationID\", \"lat\", \"lon\") \\\n",
    "    .withColumnRenamed(\"lat\", \"dropoff_lat\") \\\n",
    "    .withColumnRenamed(\"lon\", \"dropoff_lon\")\n",
    "\n",
    "# Asigno alias a los DataFrames antes de la unión\n",
    "uber_df_alias = uber_df.alias(\"uber\")\n",
    "taxi_zones_df_filtered_alias = taxi_zones_df_filtered.alias(\"taxi_zones\")\n",
    "\n",
    "# Realizo la unión usando los alias y eliminando columnas redundantes\n",
    "uber_df = uber_df_alias.join(\n",
    "    taxi_zones_df_filtered_alias,\n",
    "    uber_df_alias[\"DOLocationID\"] == taxi_zones_df_filtered_alias[\"LocationID\"],\n",
    "    how=\"left\"\n",
    ").drop(taxi_zones_df_filtered_alias[\"LocationID\"])  # Eliminar LocationID redundante\n",
    "\n",
    "# Imprimo en pantalla el resultado final\n",
    "uber_df.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "7a4220b6-970a-4318-8ce5-b5901c4cdaee",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "display(uber_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "f39bb2f5-b005-4008-a068-8ddec6b7b9ab",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Limpio el DataFrame de Uber y se calculan las columnas requeridas\n",
    "uber_cleaned = uber_df.select(\n",
    "    # Coordenadas de salida (latitud y longitud)\n",
    "    F.col(\"pickup_lat\").alias(\"pickup_latitude\"),\n",
    "    F.col(\"pickup_lon\").alias(\"pickup_longitude\"),\n",
    "    # ID de localización\n",
    "    F.col(\"PULocationID\").alias(\"pickup_location_id\"),\n",
    "    F.col(\"DOLocationID\").alias(\"dropoff_location_id\"),\n",
    "\n",
    "    # Coordenadas de llegada (latitud y longitud)\n",
    "    F.col(\"dropoff_lat\").alias(\"dropoff_latitude\"),\n",
    "    F.col(\"dropoff_lon\").alias(\"dropoff_longitude\"),\n",
    "\n",
    "    # Fecha y hora de la solicitud\n",
    "    F.col(\"request_datetime\").alias(\"request_datetime\"),\n",
    "\n",
    "    # Precio total sumando las tarifas y tasas, utilizando coalesce para manejar posibles NULLs\n",
    "    (\n",
    "        F.coalesce(F.col(\"base_passenger_fare\"), F.lit(0)) +\n",
    "        F.coalesce(F.col(\"tolls\"), F.lit(0)) +\n",
    "        F.coalesce(F.col(\"bcf\"), F.lit(0)) +\n",
    "        F.coalesce(F.col(\"sales_tax\"), F.lit(0)) +\n",
    "        F.coalesce(F.col(\"congestion_surcharge\"), F.lit(0)) +\n",
    "        F.coalesce(F.col(\"airport_fee\"), F.lit(0))\n",
    "    ).alias(\"total_price\"),\n",
    "\n",
    "    # Retraso en minutos (calculado como la diferencia entre pickup_datetime y request_datetime)\n",
    "    ((F.unix_timestamp(F.col(\"pickup_datetime\")) - F.unix_timestamp(F.col(\"request_datetime\"))) / 60).alias(\"delay_minutes\"),\n",
    "\n",
    "    # Tiempo total del viaje en minutos (calculado como la diferencia entre dropoff_datetime y request_datetime)\n",
    "    ((F.unix_timestamp(F.col(\"dropoff_datetime\")) - F.unix_timestamp(F.col(\"pickup_datetime\"))) / 60).alias(\"total_trip_time_minutes\"),\n",
    "\n",
    "    # Licencia (tipo de hide-railing)\n",
    "    F.col(\"hvfhs_license_num\").alias(\"license\"),\n",
    "\n",
    "    # Conversión de millas a kilómetros\n",
    "    (F.col(\"trip_miles\") * 1.60934).alias(\"trip_kilometers\"),\n",
    "    # Hora del día extraída de request_datetime\n",
    "    F.hour(F.col(\"request_datetime\")).alias(\"hour_of_day\"),\n",
    "\n",
    "    # Día de la semana extraído de request_datetime (1 = Domingo, 7 = Sábado)\n",
    "    F.dayofweek(F.col(\"request_datetime\")).alias(\"day_of_week\"),\n",
    "    F.to_date(F.col(\"request_datetime\")).alias(\"date\")\n",
    ")\n",
    "# Se eliminan los valroes valtantes derivado de la falta de datos de la referenciación geográfica de algunos id zonales\n",
    "uber_cleaned = uber_cleaned.dropna(subset=[\"dropoff_longitude\"])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "9f028e1e-e141-4324-af9e-11b5e6bb03a5",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "\n",
    "# Rango de fechas\n",
    "fecha_min = \"2024-01-01\"\n",
    "fecha_max = \"2024-07-01\"\n",
    "\n",
    "# Me aseguro de que las fechas estén en formato adecuado\n",
    "uber_cleaned = uber_cleaned.withColumn(\"date\", to_date(col(\"date\"), \"yyyy-MM-dd\"))\n",
    "aggregated_events_df = aggregated_events_df.withColumn(\"Date\", to_date(col(\"Date\"), \"yyyy-MM-dd\"))\n",
    "\n",
    "# Filtro por rango de fechas\n",
    "uber_cleaned = uber_cleaned.filter((col(\"date\") >= lit(fecha_min)) & (col(\"date\") <= lit(fecha_max)))\n",
    "aggregated_events_df = aggregated_events_df.filter((col(\"Date\") >= lit(fecha_min)) & (col(\"Date\") <= lit(fecha_max)))\n",
    "\n",
    "# Ajusto el formato de la hora para que sean equivalentes\n",
    "# Primero convierto  `hour_of_day` (entero) a formato \"14:00\"\n",
    "uber_cleaned = uber_cleaned.withColumn(\"hour_of_day_formatted\", concat(col(\"hour_of_day\").cast(\"string\"), lit(\":00\")))\n",
    "\n",
    "# Y después `Hour` (en formato \"14:00\") a entero \"14\"\n",
    "aggregated_events_df = aggregated_events_df.withColumn(\"Hour_int\", substring(col(\"Hour\"), 1, 2).cast(\"int\"))\n",
    "\n",
    "# Renombro columnas con espacios o caracteres especiales\n",
    "aggregated_events_df = aggregated_events_df.withColumnRenamed(\"nº events\", \"num_events\")\n",
    "print(f\"Registros antes del join: {uber_cleaned.count()}\")\n",
    "# Realizo un left join entre uber_cleaned y aggregated_events_df\n",
    "uber_cleaned = uber_cleaned.join(\n",
    "    aggregated_events_df,\n",
    "    (uber_cleaned.date == aggregated_events_df.Date) & (uber_cleaned.hour_of_day == aggregated_events_df.Hour_int),\n",
    "    how=\"left\"\n",
    ")\n",
    "print(f\"Registros después del join: {uber_cleaned.count()}\")\n",
    "\n",
    "# Relleno los valores null en la columna 'num_events' con 0\n",
    "uber_cleaned = uber_cleaned.fillna({\"num_events\": 0})\n",
    "\n",
    "# Renombro de vuelta la columna `num_events` a `nº events`\n",
    "uber_cleaned = uber_cleaned.withColumnRenamed(\"num_events\", \"nº events\")\n",
    "\n",
    "# Imprimo en pantalla el df combinado\n",
    "display(uber_cleaned)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "c809caa9-56fb-47a4-b1fc-e0ec0d33b745",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "weather_df = weather_df.withColumnRenamed(\"index\", \"time\")\n",
    "\n",
    "# Me aseguro de que el tipo de dato es timestamp\n",
    "weather_df = weather_df.withColumn(\"time\", F.col(\"time\").cast(\"timestamp\"))\n",
    "weather_df.fillna(0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "ed87695a-d74d-4276-bf3a-65bb1e2f2568",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "\n",
    "# Pongo en formato string la columna 'coco'\n",
    "weather_df = weather_df.withColumn(\"coco\", F.col(\"coco\").cast(\"string\"))\n",
    "\n",
    "# Extraigo los valores únicos de la columna 'coco' para crear una columna por cada valor\n",
    "unique_values = weather_df.select(\"coco\").distinct().rdd.flatMap(lambda x: x).collect()\n",
    "\n",
    "# Creo dinámicamente una columna para cada valor único en 'coco'\n",
    "for value in unique_values:\n",
    "    weather_df = weather_df.withColumn(\n",
    "        f\"coco_{value}\",\n",
    "        F.when(F.col(\"coco\") == value, 1).otherwise(0)\n",
    "    )\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "795ce32a-63c9-4609-a90a-ec1c2391c85e",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Realizo la unión, cruzando los datos por fecha y hora\n",
    "uber_weather_df = uber_cleaned.join(\n",
    "    weather_df,\n",
    "    (uber_cleaned[\"request_datetime\"] >= weather_df[\"time\"]) & (uber_cleaned[\"request_datetime\"] < (weather_df[\"time\"] + F.expr(\"INTERVAL 1 HOUR\"))),\n",
    "    how=\"left\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "92ecd382-a8a5-425c-a7d1-6a09e817e3db",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Filtro filas donde las tres columnas tienen valores mayores o iguales a 0 (valores que se presuponen imposibles)\n",
    "uber_weather_df = uber_weather_df[\n",
    "    (uber_weather_df['total_price'] >= 0) &\n",
    "    (uber_weather_df['delay_minutes'] >= 0) &\n",
    "    (uber_weather_df['total_trip_time_minutes'] >= 0)\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "cf41e7ca-e0e9-4c47-b3a6-f459034e9f57",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "# Calculo el tiempo total de viaje en horas\n",
    "uber_weather_df = uber_weather_df.withColumn(\n",
    "    'total_trip_time_hours', \n",
    "    uber_weather_df['total_trip_time_minutes'] / 60\n",
    ")\n",
    "\n",
    "# Calculo la velocidad promedio en km/h\n",
    "uber_weather_df = uber_weather_df.withColumn(\n",
    "    'speed_kmh', \n",
    "    uber_weather_df['trip_kilometers'] / uber_weather_df['total_trip_time_hours']\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "c9475cbb-5188-4adc-b30a-51daff074135",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "\n",
    "# Guardo el df en memoria para acelerar operaciones\n",
    "uber_weather_df = uber_weather_df.persist(StorageLevel.MEMORY_AND_DISK)\n",
    "\n",
    "# Calculo los cuartiles e IQR solo para la columna de precio\n",
    "Q1_price, Q3_price = uber_weather_df.approxQuantile(\"total_price\", [0.25, 0.75], 0.05)\n",
    "IQR_price = Q3_price - Q1_price\n",
    "\n",
    "# Filtro valores atípicos y establecer límites lógicos de acuerdo a documento\n",
    "uber_weather_df = uber_weather_df.filter(\n",
    "    (col(\"trip_kilometers\") <= 30) &  # Distancia máxima lógica para Manhattan\n",
    "    (col(\"speed_kmh\") <= 80) &         # Velocidad máxima permitida\n",
    "    (col(\"total_trip_time_minutes\") <= 120) &  # Tiempo total máximo permitido\n",
    "    (col(\"total_price\") >= (Q1_price - 1.5 * IQR_price)) &  # Precio dentro del rango intercuartílico extendido\n",
    "    (col(\"total_price\") <= 200) &    # Máximo precio  es 200 USD\n",
    "    (col(\"delay_minutes\") <= 120) &  # Máximo retraso permitido\n",
    "    (col(\"total_trip_time_minutes\") >= 0)  # Tiempo de viaje no negativo\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "b0833c6a-0d70-4126-91b9-5c8f0fe65b97",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Agrupo los datos por ubicación, hora y día de la semana\n",
    "graph_df = uber_weather_df.groupBy(\n",
    "    \"pickup_location_id\", \"dropoff_location_id\", \"hour_of_day\", \"day_of_week\"\n",
    ").agg(\n",
    "    F.avg(\"total_trip_time_minutes\").alias(\"avg_trip_time_minutes\"),\n",
    "    F.avg(\"temp\").alias(\"avg_temp\"),\n",
    "    F.avg(\"trip_kilometers\").alias(\"avg_distance\"),\n",
    "    F.avg(\"rhum\").alias(\"avg_rhum\"),\n",
    "    F.avg(\"speed_kmh\").alias(\"avg_speed_kmh\"),\n",
    "    F.avg(\"delay_minutes\").alias(\"avg_delay_minutes\"),\n",
    "    F.avg(\"prcp\").alias(\"avg_prcp\"),\n",
    "    F.avg(\"nº events\").alias(\"avg_events\"),\n",
    "    F.count(\"*\").alias(\"demand_count\")\n",
    "\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "8fa7087f-49f5-41e7-a68c-ae0fbde5c06e",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "\n",
    "# Agrupo los datos por ubicación, hora, día de la semana y demás columnas\n",
    "graph_df_2 = uber_weather_df.groupBy(\n",
    "    \"pickup_location_id\", \n",
    "    \"dropoff_location_id\", \n",
    "    \"hour_of_day\", \n",
    "    \"day_of_week\",\n",
    "    \n",
    "    # Mantengo las columnas de eventos sin agregación\n",
    "    \"Athletic Race / Tour\", \n",
    "    \"BID Multi-Block\",\n",
    "    \"Bike the Block\", \n",
    "    \"Block Party\", \n",
    "    \"Clean-Up\", \n",
    "    \"Farmers Market\", \n",
    "    \"Grid Request\", \n",
    "    \"Health Fair\", \n",
    "    \"Open Culture\", \n",
    "    \"Open Street Partner Event\", \n",
    "    \"Parade\", \n",
    "    \"Plaza Event\", \n",
    "    \"Plaza Partner Event\", \n",
    "    \"Press Conference\", \n",
    "    \"Production Event\", \n",
    "    \"Religious Event\", \n",
    "    \"Sidewalk Sale\", \n",
    "    \"Single Block Festival\", \n",
    "    \"Special Event\", \n",
    "    \"Sport - Adult\", \n",
    "    \"Sport - Youth\", \n",
    "    \"Stationary Demonstration\", \n",
    "    \"Stickball\", \n",
    "    \"Street Event\", \n",
    "    \"Street Festival\", \n",
    "    \"Theater Load in and Load Outs\",\n",
    "\n",
    "    # Mantengo las columnas de coco sin agregación\n",
    "    \"`coco_1.0`\",\n",
    "    \"`coco_20.0`\",\n",
    "    \"`coco_15.0`\",\n",
    "    \"`coco_17.0`\",\n",
    "    \"`coco_9.0`\",\n",
    "    \"`coco_12.0`\",\n",
    "    \"`coco_18.0`\",\n",
    "    \"`coco_5.0`\",\n",
    "    \"`coco_4.0`\",\n",
    "    \"`coco_7.0`\",\n",
    "    \"`coco_2.0`\",\n",
    "    \"`coco_8.0`\",\n",
    "    \"`coco_3.0`\",\n",
    "    \"`coco_16.0`\",\n",
    "    \"`coco_13.0`\"\n",
    ").agg(\n",
    "    # Promedio sobre los tiempos de viaje\n",
    "    F.avg(\"total_trip_time_minutes\").alias(\"avg_trip_time_minutes\"),\n",
    "    F.avg(\"total_trip_time_hours\").alias(\"avg_trip_time_hours\"),\n",
    "    \n",
    "    # Condiciones meteorológicas \n",
    "    F.avg(\"temp\").alias(\"avg_temp\"),\n",
    "    F.avg(\"rhum\").alias(\"avg_rhum\"),\n",
    "    F.avg(\"prcp\").alias(\"avg_prcp\"),\n",
    "    F.avg(\"snow\").alias(\"avg_snow\"),\n",
    "    F.avg(\"wspd\").alias(\"avg_wspd\"),\n",
    "    F.avg(\"wpgt\").alias(\"avg_wpgt\"),\n",
    "    F.avg(\"pres\").alias(\"avg_pres\"),\n",
    "    F.avg(\"tsun\").alias(\"avg_tsun\"),\n",
    "    \n",
    "    # Variables adicionales de distancia, velocidad y demanda\n",
    "    F.avg(\"trip_kilometers\").alias(\"avg_distance\"),\n",
    "    F.avg(\"speed_kmh\").alias(\"avg_speed_kmh\"),\n",
    "    F.avg(\"delay_minutes\").alias(\"avg_delay_minutes\"),\n",
    "    F.avg(\"total_price\").alias(\"avg_total_price\"),\n",
    "    F.count(\"*\").alias(\"demand_count\")  # Número de viajes en ese período\n",
    ")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Aquí comienza el enfoque 2 reocogido en el documento"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "cc3619cd-3735-46df-9e24-028bed17e303",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Convierto graph_df_2 a pandas para facilitar la manipulación de datos con PyTorch\n",
    "graph_df_pandas = graph_df_2.toPandas()\n",
    "\n",
    "# Calculo estadísticas de tráfico\n",
    "traffic_stats = graph_df_pandas.groupby(['pickup_location_id', 'dropoff_location_id']).agg(\n",
    "    mean_speed=('avg_speed_kmh', 'mean'),\n",
    "    max_speed=('avg_speed_kmh', 'max'),\n",
    "    min_speed=('avg_speed_kmh', 'min')\n",
    ").reset_index()\n",
    "\n",
    "# Verifico las primeras filas de traffic_stats para asegurarte de que las columnas están presentes\n",
    "print(traffic_stats.head())\n",
    "\n",
    "# Uno estadísticas de vuelta al dataframe original\n",
    "graph_df_pandas = graph_df_pandas.merge(traffic_stats, on=['pickup_location_id', 'dropoff_location_id'], how='left')\n",
    "\n",
    "# Función para asignar categorías de tráfico binario (fluido o denso)\n",
    "def assign_traffic_category(row):\n",
    "    if row['avg_speed_kmh'] >= row['mean_speed']:\n",
    "        return 0  # Tráfico fluido\n",
    "    else:\n",
    "        return 1  # Tráfico denso\n",
    "    \n",
    "\n",
    "# Asigno las categorías de tráfico al df\n",
    "graph_df_pandas['traffic_category'] = graph_df_pandas.apply(assign_traffic_category, axis=1)\n",
    "\n",
    "# Elimino filas con valores faltantes\n",
    "graph_df_pandas = graph_df_pandas.dropna()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "9910b4bb-07fe-4175-8f0a-e513e1795805",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "print(graph_df_pandas['traffic_category'].value_counts(normalize=True))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "071651a7-be67-4e44-b48d-9de36879a773",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "\n",
    "# Me aseguro que todas las columnas requeridas estén en el DataFrame\n",
    "required_columns = [\n",
    "    'hour_of_day', 'day_of_week', 'avg_distance', 'avg_speed_kmh',\n",
    "    'avg_delay_minutes', 'avg_total_price', 'demand_count', \n",
    "    'Athletic Race / Tour', 'BID Multi-Block', 'Bike the Block', 'Block Party', 'Clean-Up', 'Farmers Market',\n",
    "    'Grid Request', 'Health Fair', 'Open Culture', 'Open Street Partner Event', 'Parade', 'Plaza Event', \n",
    "    'Plaza Partner Event', 'Press Conference', 'Production Event', 'Religious Event', 'Sidewalk Sale', \n",
    "    'Single Block Festival', 'Special Event', 'Sport - Adult', 'Sport - Youth', 'Stationary Demonstration',\n",
    "    'Stickball', 'Street Event', 'Street Festival', 'Theater Load in and Load Outs', \n",
    "    'coco_1.0', 'coco_20.0', 'coco_15.0', 'coco_17.0', 'coco_9.0', \n",
    "    'coco_12.0', 'coco_18.0', 'coco_5.0', 'coco_4.0', 'coco_7.0', 'coco_2.0', 'coco_8.0', \n",
    "    'coco_3.0', 'coco_16.0', 'coco_13.0', 'demand_count'\n",
    "]\n",
    "\n",
    "missing_columns = [col for col in required_columns if col not in graph_df_pandas.columns]\n",
    "if missing_columns:\n",
    "    raise ValueError(f\"Faltan las siguientes columnas en el DataFrame: {', '.join(missing_columns)}\")\n",
    "\n",
    "# CreaCreor un diccionario con todas las ubicaciones únicas (nodos)\n",
    "locations = pd.concat([graph_df_pandas['pickup_location_id'], graph_df_pandas['dropoff_location_id']]).unique()\n",
    "\n",
    "# Asigno un índice único a cada ubicación\n",
    "location_to_index = {loc: idx for idx, loc in enumerate(locations)}\n",
    "\n",
    "# Creo una lista de aristas (edges)\n",
    "edge_index = []\n",
    "for _, row in graph_df_pandas.iterrows():\n",
    "    pickup_idx = location_to_index[row['pickup_location_id']]\n",
    "    dropoff_idx = location_to_index[row['dropoff_location_id']]\n",
    "    edge_index.append([pickup_idx, dropoff_idx])\n",
    "\n",
    "# Convierto la lista de aristas a formato tensor (PyTorch Geometric)\n",
    "edge_index = torch.tensor(edge_index, dtype=torch.long).t().contiguous()\n",
    "\n",
    "# Escalo las características de los nodos\n",
    "scaler_features = StandardScaler()\n",
    "node_features_scaled = scaler_features.fit_transform(\n",
    "    graph_df_pandas[[ \n",
    "        'hour_of_day', 'day_of_week', \n",
    "        'Athletic Race / Tour', 'BID Multi-Block', 'Bike the Block', 'Block Party', 'Clean-Up', \n",
    "        'Farmers Market', 'Grid Request', 'Health Fair', 'Open Culture', 'Open Street Partner Event', \n",
    "        'Parade', 'Plaza Event', 'Plaza Partner Event', 'Press Conference', 'Production Event', \n",
    "        'Religious Event', 'Sidewalk Sale', 'Single Block Festival', 'Special Event', 'Sport - Adult', \n",
    "        'Sport - Youth', 'Stationary Demonstration', 'Stickball', 'Street Event', 'Street Festival', \n",
    "        'Theater Load in and Load Outs', 'coco_1.0', 'coco_20.0', 'coco_15.0', 'coco_17.0', 'coco_9.0', \n",
    "        'coco_12.0', 'coco_18.0', 'coco_5.0', 'coco_4.0', 'coco_7.0', 'coco_2.0', 'coco_8.0', \n",
    "        'coco_3.0', 'coco_16.0', 'coco_13.0', 'demand_count'\n",
    "    ]].values)\n",
    "\n",
    "# Convierto las etiquetas de tráfico a valores enteros (no escalarlas)\n",
    "labels = torch.tensor(graph_df_pandas['traffic_category'].values, dtype=torch.long)\n",
    "\n",
    "# Divido los datos en conjuntos de entrenamiento y prueba\n",
    "train_idx, test_idx = train_test_split(range(len(labels)), test_size=0.2, stratify=labels)\n",
    "\n",
    "data_train = Data(\n",
    "    x=torch.tensor(node_features_scaled[train_idx], dtype=torch.float),\n",
    "    edge_index=edge_index,\n",
    "    y=labels[train_idx]\n",
    ")\n",
    "data_test = Data(\n",
    "    x=torch.tensor(node_features_scaled[test_idx], dtype=torch.float),\n",
    "    edge_index=edge_index,\n",
    "    y=labels[test_idx]\n",
    ")\n",
    "\n",
    "# Calculo los pesos de las clases basados en su distribución\n",
    "class_counts = Counter(labels[train_idx].numpy())\n",
    "class_weights = torch.tensor([1.0 / class_counts[i] for i in range(len(class_counts))], dtype=torch.float)\n",
    "\n",
    "# Definición del modelo GNN para clasificación binaria\n",
    "class GNNModel(nn.Module):\n",
    "    def __init__(self, num_features, hidden_channels):\n",
    "        super(GNNModel, self).__init__()\n",
    "        self.conv1 = pyg_nn.GCNConv(num_features, hidden_channels)\n",
    "        self.conv2 = pyg_nn.GCNConv(hidden_channels, 2)  # Ajuste para 2 clases (fluido/denso)\n",
    "        self.relu = nn.ReLU()\n",
    "    \n",
    "    def forward(self, x, edge_index):\n",
    "        # Convolución en el grafo\n",
    "        x = self.conv1(x, edge_index)\n",
    "        x = self.relu(x)\n",
    "        x = self.conv2(x, edge_index)\n",
    "        return x\n",
    "\n",
    "# Defino el modelo\n",
    "model = GNNModel(num_features=node_features_scaled.shape[1], hidden_channels=16)\n",
    "print(model)\n",
    "\n",
    "# Defino el optimizador\n",
    "optimizer = optim.Adam(model.parameters(), lr=0.01)\n",
    "\n",
    "# Defino la función de pérdida binaria\n",
    "criterion = nn.CrossEntropyLoss(weight=class_weights)\n",
    "\n",
    "# Número de épocas\n",
    "epochs = 300\n",
    "\n",
    "# Entrenamiento\n",
    "for epoch in range(epochs):\n",
    "    model.train()\n",
    "    optimizer.zero_grad()\n",
    "    \n",
    "    # Forward pass\n",
    "    output = model(data_train.x, data_train.edge_index)\n",
    "    \n",
    "    # Calcular la pérdida\n",
    "    loss = criterion(output, data_train.y)\n",
    "    \n",
    "    # Backpropagation\n",
    "    loss.backward()\n",
    "    optimizer.step()\n",
    "    \n",
    "    if epoch % 20 == 0:\n",
    "        print(f\"Epoch {epoch}: Loss = {loss.item()}\")\n",
    "\n",
    "# Evalúo del modelo\n",
    "model.eval()\n",
    "with torch.no_grad():\n",
    "    output = model(data_test.x, data_test.edge_index)\n",
    "    predicted_labels = torch.argmax(output, dim=1)  # Predicción de clase\n",
    "\n",
    "# Calculo la matriz de confusión\n",
    "conf_matrix = confusion_matrix(data_test.y.numpy(), predicted_labels.numpy())\n",
    "\n",
    "# Calculo otras métricas de clasificación\n",
    "accuracy_gnn_1 = accuracy_score(data_test.y.numpy(), predicted_labels.numpy())\n",
    "f1_1 = f1_score(data_test.y.numpy(), predicted_labels.numpy(), average='weighted')\n",
    "precision_1 = precision_score(data_test.y.numpy(), predicted_labels.numpy(), average='weighted')\n",
    "recall_1 = recall_score(data_test.y.numpy(), predicted_labels.numpy(), average='weighted')\n",
    "\n",
    "# Imprimo en pantalla las métricas\n",
    "print(\"Confusion Matrix:\")\n",
    "print(conf_matrix)\n",
    "print(f\"Accuracy: {accuracy_gnn_1}\")\n",
    "print(f\"F1 Score: {f1_1}\")\n",
    "print(f\"Precision: {precision_1}\")\n",
    "print(f\"Recall: {recall_1}\")\n",
    "\n",
    "# Muestro en pantalla la matriz de confusión\n",
    "sns.heatmap(conf_matrix, annot=True, fmt='d', cmap='Blues', xticklabels=['Fluido', 'Denso'], yticklabels=['Fluido', 'Denso'])\n",
    "plt.xlabel('Predicted')\n",
    "plt.ylabel('True')\n",
    "plt.title('Confusion Matrix')\n",
    "plt.show()\n",
    "\n",
    "print(classification_report(data_test.y.cpu().numpy(), predicted_labels.cpu().numpy(), target_names=[\"Fluido\", \"Denso\"]))\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "9a77c164-dff2-48ed-8e50-f4322bfb3240",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Divo  los datos para el Random Forest\n",
    "X = node_features_scaled  # Características\n",
    "y = labels.numpy()        # Etiquetas\n",
    "\n",
    "# Divido en conjunto de entrenamiento y prueba\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, stratify=y, random_state=42)\n",
    "\n",
    "# Creo el modelo de Random Forest\n",
    "rf_model = RandomForestClassifier(n_estimators=100, random_state=42, class_weight='balanced')\n",
    "\n",
    "# Entreno el modelo\n",
    "rf_model.fit(X_train, y_train)\n",
    "\n",
    "# Predicción en el conjunto de prueba\n",
    "y_pred_rf = rf_model.predict(X_test)\n",
    "\n",
    "# Se calculan las métricas para Random Forest\n",
    "conf_matrix_rf = confusion_matrix(y_test, y_pred_rf)\n",
    "accuracy_rf_1 = accuracy_score(y_test, y_pred_rf)\n",
    "f1_rf_1 = f1_score(y_test, y_pred_rf, average='weighted')\n",
    "precision_rf_1 = precision_score(y_test, y_pred_rf, average='weighted')\n",
    "recall_rf_1 = recall_score(y_test, y_pred_rf, average='weighted')\n",
    "\n",
    "# Imprimir métricas en pantalla\n",
    "print(\"Random Forest Metrics:\")\n",
    "print(f\"Accuracy: {accuracy_rf_1}\")\n",
    "print(f\"F1 Score: {f1_rf_1}\")\n",
    "print(f\"Precision: {precision_rf_1}\")\n",
    "print(f\"Recall: {recall_rf_1}\")\n",
    "\n",
    "# Muestro en pantalla la matriz de confusión para Random Forest\n",
    "sns.heatmap(conf_matrix_rf, annot=True, fmt='d', cmap='Blues', xticklabels=['Fluido', 'Denso'], yticklabels=['Fluido', 'Denso'])\n",
    "plt.xlabel('Predicted')\n",
    "plt.ylabel('True')\n",
    "plt.title('Confusion Matrix - Random Forest')\n",
    "plt.show()\n",
    "\n",
    "print(\"Classification Report - Random Forest:\")\n",
    "print(classification_report(y_test, y_pred_rf, target_names=[\"Fluido\", \"Denso\"]))\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Aquí comienza el enfoque 1 recogido en el documento"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "6b1b500e-0f64-470e-9ddb-4f6f4b87c583",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Convierto graph_df_2 a pandas para facilitar la manipulación de datos con PyTorch \n",
    "graph_df_pandas = graph_df.toPandas()\n",
    "\n",
    "# Calculo estadísticas de tráfico\n",
    "traffic_stats = graph_df_pandas.groupby(['pickup_location_id', 'dropoff_location_id']).agg(\n",
    "    mean_speed=('avg_speed_kmh', 'mean'),\n",
    "    max_speed=('avg_speed_kmh', 'max'),\n",
    "    min_speed=('avg_speed_kmh', 'min')\n",
    ").reset_index()\n",
    "\n",
    "# Me aseguro de que las columnas están presentes\n",
    "print(traffic_stats.head())\n",
    "\n",
    "# Uno estadísticas de vuelta al dataframe original\n",
    "graph_df_pandas = graph_df_pandas.merge(traffic_stats, on=['pickup_location_id', 'dropoff_location_id'], how='left')\n",
    "\n",
    "# Creo la función para asignar categorías de tráfico binario (fluido o denso)\n",
    "def assign_traffic_category(row):\n",
    "    if row['avg_speed_kmh'] >= row['mean_speed']:\n",
    "        return 0  # Tráfico fluido\n",
    "    else:\n",
    "        return 1  # Tráfico denso (incluye denso y excesivamente denso)\n",
    "\n",
    "# Asigno las categorías de tráfico al dataframe\n",
    "graph_df_pandas['traffic_category'] = graph_df_pandas.apply(assign_traffic_category, axis=1)\n",
    "\n",
    "# Elimino las filas con valores faltantes\n",
    "graph_df_pandas = graph_df_pandas.dropna()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "35a94f02-bf12-42ee-b3a4-265fe352a2b6",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "\n",
    "# Creo el diccionario de ubicaciones basado en el conjunto completo para consistencia\n",
    "locations = pd.concat([graph_df_pandas['pickup_location_id'], graph_df_pandas['dropoff_location_id']]).unique()\n",
    "location_to_index = {loc: idx for idx, loc in enumerate(locations)}\n",
    "\n",
    "# Creo la función para generar aristas y características\n",
    "scaler_features = StandardScaler()\n",
    "def process_data(data):\n",
    "    edge_index = []\n",
    "    for _, row in data.iterrows():\n",
    "        pickup_idx = location_to_index[row['pickup_location_id']]\n",
    "        dropoff_idx = location_to_index[row['dropoff_location_id']]\n",
    "        edge_index.append([pickup_idx, dropoff_idx])\n",
    "\n",
    "    edge_index = torch.tensor(edge_index, dtype=torch.long).t().contiguous()\n",
    "\n",
    "    node_features_scaled = scaler_features.fit_transform(data[['hour_of_day', 'day_of_week', 'avg_distance', 'avg_temp', 'avg_prcp', \"avg_events\", 'demand_count']].values) #'avg_events',\n",
    "    node_features = torch.tensor(node_features_scaled, dtype=torch.float)\n",
    "    labels = torch.tensor(data['traffic_category'].values, dtype=torch.long)\n",
    "\n",
    "    return node_features, edge_index, labels, node_features_scaled\n",
    "\n",
    "# Divido los datos en entrenamiento y prueba\n",
    "data_train, data_test = train_test_split(graph_df_pandas, test_size=0.2, stratify=graph_df_pandas['traffic_category'], random_state=42)\n",
    "\n",
    "# Proceso datos de entrenamiento y prueba\n",
    "node_features_train, edge_index_train, labels_train, node_features_scaled_train = process_data(data_train)\n",
    "node_features_test, edge_index_test, labels_test, node_features_scaled_test = process_data(data_test)\n",
    "\n",
    "# Creo objetos Data para PyTorch Geometric\n",
    "data_train_gnn = Data(x=node_features_train, edge_index=edge_index_train, y=labels_train)\n",
    "data_test_gnn = Data(x=node_features_test, edge_index=edge_index_test, y=labels_test)\n",
    "\n",
    "# Se define el modelo GNN\n",
    "class GNNModel(nn.Module):\n",
    "    def __init__(self, num_features, hidden_channels, num_classes):\n",
    "        super(GNNModel, self).__init__()\n",
    "        self.conv1 = pyg_nn.GCNConv(num_features, hidden_channels)\n",
    "        self.conv2 = pyg_nn.GCNConv(hidden_channels, num_classes)\n",
    "        self.relu = nn.ReLU()\n",
    "\n",
    "    def forward(self, x, edge_index):\n",
    "        x = self.conv1(x, edge_index)\n",
    "        x = self.relu(x)\n",
    "        x = self.conv2(x, edge_index)\n",
    "        return x\n",
    "\n",
    "# Instancio el modelo\n",
    "num_classes = 2\n",
    "model = GNNModel(num_features=node_features_train.shape[1], hidden_channels=16, num_classes=num_classes)\n",
    "\n",
    "# Defino optimizador y función de pérdida\n",
    "optimizer = optim.Adam(model.parameters(), lr=0.01)\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "\n",
    "# Entrenamiento\n",
    "epochs = 300\n",
    "for epoch in range(epochs):\n",
    "    model.train()\n",
    "    optimizer.zero_grad()\n",
    "\n",
    "    output = model(data_train_gnn.x, data_train_gnn.edge_index)\n",
    "\n",
    "    # Se calcula la  pérdida\n",
    "    loss = criterion(output, data_train_gnn.y)\n",
    "\n",
    "    loss.backward()\n",
    "    optimizer.step()\n",
    "\n",
    "    if epoch % 20 == 0:\n",
    "        print(f\"Epoch {epoch}: Loss = {loss.item()}\")\n",
    "\n",
    "# Se evalúa en conjunto de prueba\n",
    "model.eval()\n",
    "with torch.no_grad():\n",
    "    logits_test = model(data_test_gnn.x, data_test_gnn.edge_index)\n",
    "    predictions_test = torch.argmax(logits_test, dim=1)\n",
    "\n",
    "# Se crea la matríz de confusión\n",
    "conf_matrix_gnn = confusion_matrix(data_test_gnn.y.cpu().numpy(), predictions_test.cpu().numpy())\n",
    "sns.heatmap(conf_matrix_gnn, annot=True, fmt='d', cmap='Blues', xticklabels=[\"Fluido\", \"Denso\"],\n",
    "            yticklabels=[\"Fluido\", \"Denso\"])\n",
    "plt.xlabel(\"Predicted\")\n",
    "plt.ylabel(\"Actual\")\n",
    "plt.title(\"Confusion Matrix - GNN\")\n",
    "plt.show()\n",
    "# Calculo otras métricas de clasificación\n",
    "accuracy_gnn_2 = accuracy_score(data_test_gnn.y.numpy(), predictions_test.numpy())\n",
    "f1_2 = f1_score(data_test_gnn.y.numpy(), predictions_test.numpy(), average='weighted')\n",
    "precision_2 = precision_score(data_test_gnn.y.numpy(), predictions_test.numpy(), average='weighted')\n",
    "recall_2 = recall_score(data_test_gnn.y.numpy(), predictions_test.numpy(), average='weighted')\n",
    "# Reporte de clasificación para predictions_test\n",
    "print(\"Classification Report - GNN:\")\n",
    "print(classification_report(data_test_gnn.y.cpu().numpy(), predictions_test.cpu().numpy(), target_names=[\"Fluido\", \"Denso\"]))\n",
    "\n",
    "# Comparo con el Random Forest\n",
    "X_train, X_test = node_features_scaled_train, node_features_scaled_test\n",
    "y_train, y_test = labels_train.numpy(), labels_test.numpy()\n",
    "\n",
    "rf_model = RandomForestClassifier(n_estimators=100, random_state=42, class_weight='balanced')\n",
    "rf_model.fit(X_train, y_train)\n",
    "\n",
    "# Predicción en el conjunto de prueba\n",
    "y_pred_rf = rf_model.predict(X_test)\n",
    "\n",
    "# Calculo las métricas para Random Forest\n",
    "conf_matrix_rf = confusion_matrix(y_test, y_pred_rf)\n",
    "accuracy_rf_2 = accuracy_score(y_test, y_pred_rf)\n",
    "f1_rf_2 = f1_score(y_test, y_pred_rf, average='weighted')\n",
    "precision_rf_2 = precision_score(y_test, y_pred_rf, average='weighted')\n",
    "recall_rf_2 = recall_score(y_test, y_pred_rf, average='weighted')\n",
    "\n",
    "# Imprimo las métricas en pantalla para el rf\n",
    "print(\"Random Forest Metrics:\")\n",
    "print(f\"Accuracy: {accuracy_rf_2}\")\n",
    "print(f\"F1 Score: {f1_rf_2}\")\n",
    "print(f\"Precision: {precision_rf_2}\")\n",
    "print(f\"Recall: {recall_rf_2}\")\n",
    "\n",
    "# Imprimo las métricas en pantalla para la gnn\n",
    "print(\"GNN Metrics:\")\n",
    "print(f\"Accuracy: {accuracy_gnn_2}\")\n",
    "print(f\"F1 Score: {f1_2}\")\n",
    "print(f\"Precision: {precision_2}\")\n",
    "print(f\"Recall: {recall_2}\")\n",
    "\n",
    "# Imprimo en pantalla la matriz de confusión para el rf\n",
    "sns.heatmap(conf_matrix_rf, annot=True, fmt='d', cmap='Blues', xticklabels=['Fluido', 'Denso'], yticklabels=['Fluido', 'Denso'])\n",
    "plt.xlabel('Predicted')\n",
    "plt.ylabel('True')\n",
    "plt.title('Confusion Matrix - Random Forest')\n",
    "plt.show()\n",
    "\n",
    "print(\"Classification Report - Random Forest:\")\n",
    "print(classification_report(y_test, y_pred_rf, target_names=[\"Fluido\", \"Denso\"]))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "4868748e-c09f-4e6f-bf6f-c7fcd8fb92c7",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Aquí se genera el gráfico de comparativa entre modelos y enfoques\n",
    "# Establezco las etiquetas y posiciones de las métricas\n",
    "metrics_labels = [\"Accuracy\", \"F1 Score\", \"Precision\", \"Recall\"]\n",
    "x = np.arange(len(metrics_labels))  # posiciones de las métricas\n",
    "\n",
    "# Configuro el gráfico\n",
    "width = 0.2  # es el ancho de las barras\n",
    "\n",
    "# Creo el subgráfico para cada enfoque\n",
    "fig, ax = plt.subplots(figsize=(10, 6))\n",
    "\n",
    "# Creo como tal los gráficos de barras para cada modelo y enfoque\n",
    "ax.bar(x - 1.5 * width, [accuracy_gnn_1, f1_1, precision_1, recall_1], width, label='GNN Enfoque 1', color='skyblue')\n",
    "ax.bar(x - 0.5 * width, [accuracy_rf_1, f1_rf_1, precision_rf_1, recall_rf_1], width, label='RF Enfoque 1', color='lightgreen')\n",
    "ax.bar(x + 0.5 * width, [accuracy_gnn_2, f1_2, precision_2, recall_2], width, label='GNN Enfoque 2', color='dodgerblue')\n",
    "ax.bar(x + 1.5 * width, [accuracy_rf_2, f1_rf_2, precision_rf_2, recall_rf_2], width, label='RF Enfoque 2', color='seagreen')\n",
    "\n",
    "# Configuración del gráfico\n",
    "ax.set_xlabel(\"Métricas\")\n",
    "ax.set_ylabel(\"Valores\")\n",
    "ax.set_title(\"Comparación de Métricas entre Modelos y Enfoques\")\n",
    "ax.set_xticks(x)\n",
    "ax.set_xticklabels(metrics_labels)\n",
    "ax.legend()\n",
    "\n",
    "# Establezco los limites entre 0 y 1 del gráfico, siendo 1 el 100%\n",
    "ax.set_ylim(0, 1)\n",
    "\n",
    "# Imprimo en pantalla el gráfico\n",
    "plt.tight_layout()\n",
    "plt.show()\n"
   ]
  }
 ],
 "metadata": {
  "application/vnd.databricks.v1+notebook": {
   "computePreferences": null,
   "dashboards": [],
   "environmentMetadata": {
    "base_environment": "",
    "client": "1"
   },
   "language": "python",
   "notebookMetadata": {
    "pythonIndentUnit": 4
   },
   "notebookName": "neural_network_traffic_predicction",
   "widgets": {}
  },
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
